{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Evaluation Lab\n",
    "\n",
    "Complete the exercises below to solidify your knowledge and understanding of supervised learning model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = pd.DataFrame(data[\"target\"], columns=['MEDV'])\n",
    "\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `MEDV` field represents the median value of owner-occupied homes (in $1000's) and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a `LinearRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate and print R-squared for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test R-squared 0.742568605367234 Train R-squared 0.7357477229715652\n"
     ]
    }
   ],
   "source": [
    "r2_test = metrics.r2_score(y_test, y_test_pred)\n",
    "r2_train = metrics.r2_score(y_train, y_train_pred)\n",
    "print('Test R-squared',r2_test, 'Train R-squared',r2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate and print mean squared error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test MSE 20.628596486908137 Train MSE 22.58619095400517\n"
     ]
    }
   ],
   "source": [
    "mse_test = metrics.mean_squared_error(y_test, y_test_pred)\n",
    "mse_train = metrics.mean_squared_error(y_train, y_train_pred)\n",
    "print('Test MSE',mse_test, 'Train MSE',mse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate and print mean absolute error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test MAE 3.4821531166284685 Train MAE 3.371862005656369\n"
     ]
    }
   ],
   "source": [
    "mae_test = metrics.mean_absolute_error(y_test, y_test_pred)\n",
    "mae_train = metrics.mean_absolute_error(y_train, y_train_pred)\n",
    "print('Test MAE',mae_test, 'Train MAE',mae_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = pd.DataFrame(data[\"target\"], columns=[\"class\"])\n",
    "\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `class` field represents the type of flower and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', axis = 1)\n",
    "y = data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a `LogisticRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_pred_log = logreg.predict(X_test)\n",
    "y_train_pred_log = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate and print the accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy 0.9666666666666667 Train Accuracy 0.975\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = metrics.accuracy_score(y_test, y_test_pred_log)\n",
    "accuracy_train = metrics.accuracy_score(y_train, y_train_pred_log)\n",
    "print('Test Accuracy',accuracy_test, 'Train Accuracy',accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate and print the balanced accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Balanced Accuracy 0.9743589743589745 Train Balanced Accuracy 0.9744062244062244\n"
     ]
    }
   ],
   "source": [
    "balanced_accuracy_test = metrics.balanced_accuracy_score(y_test, y_test_pred_log)\n",
    "balanced_accuracy_train = metrics.balanced_accuracy_score(y_train, y_train_pred_log)\n",
    "print('Test Balanced Accuracy',balanced_accuracy_test, 'Train Balanced Accuracy',balanced_accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calculate and print the precision score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "macro\n\nTest Precision Score 0.9523809523809524 Train Precision Score 0.975925925925926\nmicro\n\nTest Precision Score 0.9666666666666667 Train Precision Score 0.975\nweighted\n\nTest Precision Score 0.9714285714285714 Train Precision Score 0.975138888888889\n"
     ]
    }
   ],
   "source": [
    "average_type = [ 'macro', 'micro', 'weighted']\n",
    "\n",
    "for ave in average_type:\n",
    "    precision_score_test = metrics.precision_score(y_test, y_test_pred_log, average = ave)\n",
    "    precision_score_train = metrics.precision_score(y_train, y_train_pred_log, average = ave)\n",
    "    print(f'{ave}\\n')\n",
    "    print('Test Precision Score',precision_score_test, 'Train Precision Score', precision_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate and print the recall score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "macro\n\nTest Recall Score 0.9743589743589745 Train Recall Score 0.9744062244062244\nmicro\n\nTest Recall Score 0.9666666666666667 Train Recall Score 0.975\nweighted\n\nTest Recall Score 0.9666666666666667 Train Recall Score 0.975\n"
     ]
    }
   ],
   "source": [
    "average_type = [ 'macro', 'micro', 'weighted']\n",
    "\n",
    "for ave in average_type:\n",
    "    recall_score_test = metrics.recall_score(y_test, y_test_pred_log, average = ave)\n",
    "    recall_score_train = metrics.recall_score(y_train, y_train_pred_log, average = ave)\n",
    "    print(f'{ave}\\n')\n",
    "    print('Test Recall Score',recall_score_test, 'Train Recall Score', recall_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calculate and print the F1 score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "macro\n\nTest F1 Score 0.9610256410256409 Train F1 Score 0.9750654148068341\nmicro\n\nTest F1 Score 0.9666666666666667 Train F1 Score 0.975\nweighted\n\nTest F1 Score 0.9672820512820514 Train F1 Score 0.9749692165614899\n"
     ]
    }
   ],
   "source": [
    "average_type = [ 'macro', 'micro', 'weighted']\n",
    "\n",
    "for ave in average_type:\n",
    "    f1_score_test = metrics.f1_score (y_test, y_test_pred_log, average = ave)\n",
    "    f1_score_train = metrics.f1_score(y_train, y_train_pred_log, average = ave)\n",
    "    print(f'{ave}\\n')\n",
    "    print('Test F1 Score',f1_score_test, 'Train F1 Score', f1_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate confusion matrices for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Setosa  Versicolour  Virginica\n",
       "Setosa           11            0          0\n",
       "Versicolour       0            6          0\n",
       "Virginica         0            1         12"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Setosa</th>\n      <th>Versicolour</th>\n      <th>Virginica</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Setosa</th>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Versicolour</th>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Virginica</th>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "names = ['Setosa','Versicolour', 'Virginica']\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_test_pred_log), columns = names, index = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Setosa  Versicolour  Virginica\n",
       "Setosa           39            0          0\n",
       "Versicolour       0           43          1\n",
       "Virginica         0            2         35"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Setosa</th>\n      <th>Versicolour</th>\n      <th>Virginica</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Setosa</th>\n      <td>39</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Versicolour</th>\n      <td>0</td>\n      <td>43</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Virginica</th>\n      <td>0</td>\n      <td>2</td>\n      <td>35</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(y_train, y_train_pred_log), columns = names, index = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: For each of the data sets in this lab, try training with some of the other models you have learned about, recalculate the evaluation metrics, and compare to determine which models perform best on each data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}